{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeCLUTR\n",
    "We present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations, a self-supervised method for learning universal sentence embeddings that transfer to a wide variety of natural language processing (NLP) tasks. Our objective leverages recent advances in deep metric learning (DML) and has the advantage of being conceptually simple and easy to implement, requiring no specialized architectures or labelled training data. We demonstrate that our objective can be used to pretrain transformers to state-of-the-art performance on SentEval, a popular benchmark for evaluating universal sentence embeddings, outperforming existing supervised, semi-supervised and unsupervised methods. We perform extensive ablations to determine which factors contribute to the quality of the learned embeddings. Our code will be publicly available and can be easily adapted to new datasets or used to embed unseen text. \n",
    "\n",
    "!pip install git+https://github.com/JohnGiorgi/DeCLUTR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from declutr import Encoder\n",
    "pretrained_model_or_path = \"declutr-small\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda_device = torch.cuda.current_device()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    cuda_device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------------------- DATA -------------------------------------\n",
    "\n",
    "DATA_PATH = \"../data/final_tweets/\"\n",
    "\n",
    "train_df = pd.read_csv(DATA_PATH +'train_df.csv')\n",
    "valid_df = pd.read_csv(DATA_PATH +'validate_df.csv')\n",
    "test_df  = pd.read_csv(DATA_PATH + 'test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['tweet_text']\n",
    "Y_train = train_df['text_info']\n",
    "\n",
    "X_valid = valid_df['tweet_text']\n",
    "Y_valid = valid_df['text_info']\n",
    "\n",
    "train_full_df = pd.concat([X_train, X_valid])\n",
    "Y_train_val  = pd.concat([Y_train, Y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = list(X_valid.values)\n",
    "tweet10 = tweet_text[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(pretrained_model_or_path, cuda_device=cuda_device)\n",
    "embeddings = encoder(tweet10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: text_info, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_valid[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finally turntable running irma to celebrate i thought i play dancin tunes haim',\n",
       " 'couple blow away irma amp harvey schluter happy 75th wedding anniversary',\n",
       " 'state tab irma already rising flapol',\n",
       " 'rt looking ice food hot meals irma here find',\n",
       " '5 am edt sep 27 key messages tropical storm maria',\n",
       " 'hurricane insurance claims faq answered experts maria irma',\n",
       " 'cyclone mora adds rohingya plight bangladesh',\n",
       " 'rt we survived once we can survive again hurricaneharvey',\n",
       " 'prepare mobile command vehicles deployment assist hurricane maria',\n",
       " 'donate underwear harvey victims get free cup coffee cobb']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at johngiorgi/declutr-small were not used when initializing RobertaModel: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.496265172958374\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"johngiorgi/declutr-small\")\n",
    "model = AutoModel.from_pretrained(\"johngiorgi/declutr-small\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare some text to embed\n",
    "text = ['finally turntable running irma to celebrate i thought i play dancin tunes haim',\n",
    " 'couple blow away irma amp harvey schluter happy 75th wedding anniversary',\n",
    " 'state tab irma already rising flapol',\n",
    " 'rt looking ice food hot meals irma here find',\n",
    " '5 am edt sep 27 key messages tropical storm maria',\n",
    " 'hurricane insurance claims faq answered experts maria irma',\n",
    " 'cyclone mora adds rohingya plight bangladesh',\n",
    " 'rt we survived once we can survive again hurricaneharvey',\n",
    " 'prepare mobile command vehicles deployment assist hurricane maria',\n",
    " 'donate underwear harvey victims get free cup coffee cobb']\n",
    "\n",
    "\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "# Put the tensors on the GPU, if available\n",
    "for name, tensor in inputs.items():\n",
    "    inputs[name] = tensor.to(model.device)\n",
    "\n",
    "# Embed the text\n",
    "with torch.no_grad():\n",
    "    sequence_output, _ = model(**inputs, output_hidden_states=False)\n",
    "\n",
    "# Mean pool the token-level embeddings to get sentence-level embeddings\n",
    "embeddings = torch.sum(\n",
    "    sequence_output * inputs[\"attention_mask\"].unsqueeze(-1), dim=1\n",
    ") / torch.clamp(torch.sum(inputs[\"attention_mask\"], dim=1, keepdims=True), min=1e-9)\n",
    "embeddings = embeddings.cpu()\n",
    "\n",
    "# Compute a semantic similarity via the cosine distance\n",
    "semantic_sim = 1 - cosine(embeddings[0], embeddings[1])\n",
    "print(semantic_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46503373980522156\n"
     ]
    }
   ],
   "source": [
    "semantic_sim = 1 - cosine(embeddings[0], embeddings[3])\n",
    "print(semantic_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.541486382484436\n"
     ]
    }
   ],
   "source": [
    "semantic_sim = 1 - cosine(embeddings[0], embeddings[3])\n",
    "print(semantic_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
